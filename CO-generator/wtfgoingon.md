# WTF is Going On? ğŸ¤”

## TL;DR
This is an **AI-powered Course Outcome (CO) Generator** for VTU courses. It takes your PDFs/PPTs/DOCX files, processes them through a 5-stage AI pipeline, and spits out 6 perfectly formatted Course Outcomes that follow Bloom's Taxonomy and VTU guidelines.

---

## ğŸ¯ The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VTU CO GENERATOR PIPELINE                        â”‚
â”‚                                                                     â”‚
â”‚  Input: PDFs, PPTs, DOCX â†’ Output: 6 VTU-Aligned Course Outcomes  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete System Flow (ASCII Art)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   USER UPLOADS       â”‚
                    â”‚  Course Materials    â”‚
                    â”‚ (PDF/PPT/DOCX/TXT)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         STAGE 1: DOCUMENT INTELLIGENCE           â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼
   [PDF Extract]        [PPT Extract]         [DOCX Extract]
   PyPDF2/OCR           python-pptx           python-docx
        â”‚                      â”‚                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Semantic Chunking   â”‚
                    â”‚  (1000 chars chunks) â”‚
                    â”‚  + Overlap (200)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Generate Embeddings â”‚
                    â”‚  (all-MiniLM-L6-v2)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘      STAGE 2: KNOWLEDGE GRAPH CONSTRUCTION       â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Build Graph Structure                   â”‚
        â”‚                                                 â”‚
        â”‚  Nodes:                    Edges:               â”‚
        â”‚  â€¢ Modules                 â€¢ PREREQUISITE       â”‚
        â”‚  â€¢ Topics                  â€¢ CONTAINS           â”‚
        â”‚  â€¢ Bloom Levels            â€¢ REQUIRES           â”‚
        â”‚  â€¢ PO Mappings             â€¢ MAPS_TO_PO         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Store in Neo4j      â”‚
              â”‚   (or in-memory)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘        STAGE 3: GRAPH-RAG RETRIEVAL               â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                      â”‚
        â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Search    â”‚              â”‚  Graph Traversal     â”‚
â”‚  (ChromaDB)       â”‚              â”‚  (BFS on KG)         â”‚
â”‚  â€¢ HNSW Index     â”‚              â”‚  â€¢ Find related      â”‚
â”‚  â€¢ Similarity     â”‚              â”‚    topics/modules    â”‚
â”‚  â€¢ Top-k chunks   â”‚              â”‚  â€¢ PO connections    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Hybrid Fusion       â”‚
              â”‚  70% Vector +        â”‚
              â”‚  30% Graph           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       STAGE 4: MULTI-TASK LLM GENERATION          â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Fine-Tuned Language Model              â”‚
        â”‚                                                â”‚
        â”‚  Base Models:                                  â”‚
        â”‚  â€¢ Qwen 2.5 0.5B (preferred)                   â”‚
        â”‚  â€¢ GPT-Neo 125M (Mac fallback)                 â”‚
        â”‚                                                â”‚
        â”‚  Fine-tuning:                                  â”‚
        â”‚  â€¢ LoRA Adapters (r=16, Î±=32)                  â”‚
        â”‚  â€¢ Trained on VTU CO examples                  â”‚
        â”‚                                                â”‚
        â”‚  Multi-Task Outputs:                           â”‚
        â”‚  1. CO Text (15-20 words)                      â”‚
        â”‚  2. Bloom Level (Apply/Analyze/Evaluate)       â”‚
        â”‚  3. PO Mappings (PO1-PO12)                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘         STAGE 5: REFINEMENT LAYER                 â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Reward Model Scoring                   â”‚
        â”‚                                                â”‚
        â”‚  1. Conciseness Check (15-20 words)            â”‚
        â”‚  2. VTU Compliance (terminology, format)       â”‚
        â”‚  3. OBE Alignment (measurable, actionable)     â”‚
        â”‚  4. Bloom Taxonomy Accuracy                    â”‚
        â”‚  5. PO Mapping Validity                        â”‚
        â”‚                                                â”‚
        â”‚  Final Score: 0-100%                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Add Explainability  â”‚
              â”‚  â€¢ Source chunks     â”‚
              â”‚  â€¢ Graph paths       â”‚
              â”‚  â€¢ Justification     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           FINAL OUTPUT (6 COs)                 â”‚
        â”‚                                                â”‚
        â”‚  Example:                                      â”‚
        â”‚  CO1: Apply data structures to solve          â”‚
        â”‚       real-world algorithmic problems          â”‚
        â”‚       [Bloom: Apply] [PO: PO1, PO2, PO12]      â”‚
        â”‚                                                â”‚
        â”‚  + Metrics Dashboard                           â”‚
        â”‚  + Performance Report                          â”‚
        â”‚  + Quality Scores                              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Key Files & What They Do

### Core Pipeline Files
```
src/
â”œâ”€â”€ enhanced_pipeline.py          â­ Main orchestrator - runs all 5 stages
â”œâ”€â”€ metrics_dashboard.py          ğŸ¨ Streamlit UI - upload files & see results
â”œâ”€â”€ smart_co_generator.py         ğŸ§  CO generation logic
â””â”€â”€ fastapi.py                    ğŸŒ REST API (currently not active)
```

### Stage Components
```
src/
â”œâ”€â”€ document_intelligence.py      ğŸ“„ Stage 1: Extract & chunk documents
â”œâ”€â”€ extract_text.py               ğŸ“ Helper for text extraction
â”œâ”€â”€ build_chromadb.py             ğŸ’¾ Build vector database
â”‚
â”œâ”€â”€ knowledge_graph.py            ğŸ•¸ï¸  Stage 2: Build knowledge graph
â”‚
â”œâ”€â”€ graph_rag.py                  ğŸ” Stage 3: Hybrid retrieval
â”œâ”€â”€ chromadb_utils.py             ğŸ” ChromaDB search utilities
â”‚
â”œâ”€â”€ multitask_model.py            ğŸ¤– Stage 4: LLM with LoRA adapters
â”œâ”€â”€ train_lora_qwen.py            ğŸ‹ï¸  Train Qwen model
â”œâ”€â”€ train_lora_mac.py             ğŸ Train GPT-Neo (Mac optimized)
â”‚
â””â”€â”€ refinement_layer.py           âœ¨ Stage 5: Reward scoring & validation
```

### Support Files
```
src/
â”œâ”€â”€ metrics_evaluation.py         ğŸ“Š Evaluate quality metrics
â”œâ”€â”€ latency_optimizer.py          âš¡ Caching & performance optimization
â”œâ”€â”€ build_jsonl.py                ğŸ“‹ Training data generation
â”œâ”€â”€ build_better_jsonl.py         ğŸ“‹ Advanced training data
â””â”€â”€ rebuild_training_data.py      ğŸ”„ Clean & deduplicate training data
```

---

## ğŸ—‚ï¸ Data Flow Through Directories

```
/data/
â”œâ”€â”€ raw/                          ğŸ“¥ Original uploaded files (PDFs, PPTs)
â”‚   â”œâ”€â”€ syllabus.pdf
â”‚   â””â”€â”€ course_materials.pptx
â”‚
â”œâ”€â”€ extracted/                    ğŸ“ Extracted text from documents
â”‚   â”œâ”€â”€ syllabus.txt
â”‚   â””â”€â”€ course_materials.txt
â”‚
â”œâ”€â”€ jsonl/                        ğŸ“‹ Training data for LLM fine-tuning
â”‚   â”œâ”€â”€ training_data.jsonl
â”‚   â””â”€â”€ cleaned_training_data.jsonl
â”‚
â”œâ”€â”€ chroma_db/                    ğŸ’¾ Vector database (22MB, 444 chunks)
â”‚   â””â”€â”€ [ChromaDB files]
â”‚
â”œâ”€â”€ knowledge_graph.json          ğŸ•¸ï¸  Graph structure (134KB)
â”‚
â”œâ”€â”€ pipeline_report.json          ğŸ“Š Latest pipeline execution metrics
â”‚
â””â”€â”€ user_feedback.json            ğŸ’¬ Feedback for RLHF improvement

/gptneo_co_lora/                  ğŸ¤– GPT-Neo model + LoRA (59MB)
/qwen_co_lora/                    ğŸ¤– Qwen model + LoRA (98MB)
```

---

## ğŸ”§ Tech Stack

### AI & ML
- **PyTorch** - Deep learning framework
- **Transformers** - Hugging Face models (Qwen 2.5, GPT-Neo)
- **PEFT (LoRA)** - Efficient fine-tuning
- **Sentence Transformers** - Embeddings (all-MiniLM-L6-v2)

### Databases
- **ChromaDB** - Vector database (semantic search)
- **Neo4j** - Knowledge graph (optional, falls back to in-memory)

### Web Frameworks
- **Streamlit** - Interactive dashboard
- **FastAPI** - REST API (currently disabled)

### Document Processing
- **PyPDF2** - PDF extraction
- **python-pptx** - PowerPoint extraction
- **python-docx** - Word extraction
- **pytesseract** - OCR for images

---

## âš¡ Performance Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline Performance (6 COs)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Total Time:        ~3.5 seconds        â”‚
â”‚  Per CO:            ~580 milliseconds   â”‚
â”‚  Bloom Accuracy:    â‰¥85%                â”‚
â”‚  VTU Compliance:    â‰¥88%                â”‚
â”‚  OBE Alignment:     â‰¥80%                â”‚
â”‚  Overall Quality:   â‰¥82%                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ What Makes This Special?

### 1. Multi-Stage AI Pipeline
Not just a simple LLM prompt - uses 5 sophisticated stages for accuracy

### 2. Graph-RAG Hybrid
Combines vector search (ChromaDB) + knowledge graph traversal for better context

### 3. Fine-Tuned Models
Custom LoRA adapters trained on VTU CO examples (not generic GPT)

### 4. Explainable AI
Every CO comes with:
- Source chunks from documents
- Knowledge graph traversal paths
- Justification for Bloom level & PO mappings

### 5. VTU Compliance
- Follows exact VTU formatting
- Bloom's Taxonomy validated
- OBE (Outcome-Based Education) aligned
- PO (Program Outcome) mappings included

---

## ğŸš€ How to Use

### Via Streamlit Dashboard (Recommended)
```bash
streamlit run src/metrics_dashboard.py
```
1. Upload your PDF/PPT/DOCX files
2. Click "Generate COs"
3. See 6 generated COs with metrics

### Via Python Script
```python
from src.smart_co_generator import SmartCOGenerator

generator = SmartCOGenerator()
cos = generator.generate_cos("path/to/syllabus.pdf", num_cos=6)
print(cos)
```

---

## ğŸ” Example Output

```
CO1: Apply data structures and algorithms to solve computational problems
     [Bloom: Apply] [PO: PO1, PO2, PO12]
     Source: Module 1 - Data Structures (Graph path: Moduleâ†’Topicâ†’Bloom)

CO2: Analyze the time and space complexity of various sorting algorithms
     [Bloom: Analyze] [PO: PO1, PO2, PO3, PO12]
     Source: Module 2 - Algorithm Analysis

CO3: Evaluate different database normalization techniques for data integrity
     [Bloom: Evaluate] [PO: PO2, PO3, PO5]
     Source: Module 3 - Database Design

... (3 more COs)
```

---

## ğŸ§© Component Interactions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SmartCOGenerator        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â–º DocumentIntelligence (Stage 1)
         â”‚       â””â”€â–º ChromaDB (vector store)
         â”‚
         â”œâ”€â–º KnowledgeGraph (Stage 2)
         â”‚       â””â”€â–º Neo4j/In-memory graph
         â”‚
         â”œâ”€â–º GraphRAG (Stage 3)
         â”‚       â”œâ”€â–º ChromaDB search
         â”‚       â””â”€â–º Graph traversal
         â”‚
         â”œâ”€â–º MultiTaskModel (Stage 4)
         â”‚       â”œâ”€â–º Qwen 2.5 0.5B + LoRA
         â”‚       â””â”€â–º GPT-Neo 125M + LoRA
         â”‚
         â””â”€â–º RefinementLayer (Stage 5)
                 â”œâ”€â–º Reward scoring
                 â””â”€â–º Explainability

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LatencyOptimizer        â”‚
â”‚  â€¢ Embedding cache       â”‚
â”‚  â€¢ Batch processing      â”‚
â”‚  â€¢ Profiling             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MetricsEvaluation       â”‚
â”‚  â€¢ Quality scoring       â”‚
â”‚  â€¢ Compliance checks     â”‚
â”‚  â€¢ Accuracy metrics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Educational Context

This tool is designed for:
- **VTU** (Visvesvaraya Technological University) courses
- **OBE** (Outcome-Based Education) compliance
- **NBA** (National Board of Accreditation) requirements
- **Bloom's Taxonomy** classification
- **Program Outcome (PO)** mapping

---

## ğŸ“ˆ Training Pipeline

```
Raw CO Examples
       â†“
build_better_jsonl.py â†’ Generate JSONL training data
       â†“
rebuild_training_data.py â†’ Clean & deduplicate
       â†“
train_lora_qwen.py â†’ Fine-tune with LoRA
       â†“
qwen_co_lora/ â†’ Save adapter weights
       â†“
multitask_model.py â†’ Load for inference
```

---

## ğŸ³ Docker Support

```bash
docker build -t co-generator .
docker run -p 8501:8501 co-generator
```

Runs Streamlit dashboard in container with all dependencies.

---

## ğŸ’¡ Key Optimizations

1. **Embedding Cache**: LRU cache (10K entries) saves 80% on redundant embedding calls
2. **Batch Processing**: ThreadPoolExecutor parallelizes document processing
3. **Model Compilation**: `torch.compile()` speeds up inference
4. **Persistent Cache**: Embeddings stored to disk for cross-session reuse
5. **HNSW Index**: Fast approximate nearest neighbor search in ChromaDB

---

## ğŸ¯ Quality Targets

- âœ… Bloom Classification: â‰¥85% accuracy
- âœ… VTU Compliance: â‰¥88% adherence
- âœ… OBE Alignment: â‰¥80% alignment
- âœ… Overall Quality: â‰¥82% score
- âœ… Latency: <600ms per CO

---

## ğŸ”® What's Not Currently Used

- FastAPI server (`src/fastapi.py`) - commented out
- Neo4j database - falls back to in-memory graph
- Some demo files (`demo_pipeline.py`, `demo_integration.py`) - standalone demos

---

## ğŸ—ï¸ Architecture Philosophy

1. **Modularity**: Each stage is independent, can be swapped/upgraded
2. **Explainability**: Full traceability from source to output
3. **Scalability**: ChromaDB + Neo4j ready for enterprise
4. **Performance**: Optimized for speed without sacrificing quality
5. **Compliance**: VTU/OBE/NBA standards baked in

---

## ğŸ¬ Summary

This is a **production-grade AI system** that transforms course materials into accreditation-ready Course Outcomes using:
- **Graph-RAG** (cutting-edge retrieval)
- **LoRA fine-tuning** (custom VTU knowledge)
- **Multi-stage pipeline** (not just prompt engineering)
- **Explainable AI** (full justification)

Built for educational institutions needing fast, accurate, compliant CO generation at scale.

---

**Made with â¤ï¸ for VTU Faculty & Accreditation Teams**
